{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08877595-17b1-43e5-acda-60b6515254a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7003fe5-24d6-4689-903e-2ff8370d6acc",
   "metadata": {},
   "source": [
    "# Importar entorno y familiarizarse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd1c580c-9d7a-44a0-adab-d75c9a7dcd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boardgame2 import ReversiEnv\n",
    "import numpy as np\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9f0214-b031-431d-ab8b-5fb7bba5a64e",
   "metadata": {},
   "source": [
    "# Crear 3 tipos de jugador\n",
    "- Random: Selecciona uniformemente una de las acciones válidas\n",
    "- Greedy: Selecciona la acción que le da más ganancia inmediata (cantidad de piezas que come). Si hay más de una acción que da máxima ganancia samplear uniformemente entre ellas\n",
    "- Optimum (solo para 4x4): Usando resultados de la PI optima obtenida por policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9950fc-9ebe-4d31-b733-1a13047f1b87",
   "metadata": {},
   "source": [
    "Tener en cuenta que:\n",
    "- ReversiEnv tiene los métodos get_valid y next_step y no es necesario mantener el estado del entorno\n",
    "- env.PASS ([-1,  0]) es una acción valida posible y debería hacerse cuando no get_valid devuelve una matriz de ceros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3ae9f1-4f78-47e2-ac3a-1c65d57e359c",
   "metadata": {},
   "source": [
    "Para el optimo en 4x4 bajar usar la PI obtenida en la notebook anterior guardado en /mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e8269ba-fe4f-476a-b4f1-107c379f8dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from players import RandomPlayer as RandomPlayer2\n",
    "from players import GreedyPlayer as GreedyPlayer2\n",
    "from players import DictPolicyPlayer as DictPolicyPlayer2\n",
    "\n",
    "class GreedyPlayer3():\n",
    "    def __init__(self, player=1, board_shape=None, env=None, flatten_action=False):\n",
    "        if (env is None) and (board_shape is None):\n",
    "            print(\"board_shape and env can't be both None\")\n",
    "        if env is None:\n",
    "            env = ReversiEnv(board_shape=board_shape)\n",
    "        self.env = env\n",
    "        self.player = player # player number. 1 o -1\n",
    "        #self.env.player = player\n",
    "        self.flatten_action = flatten_action\n",
    "        self.board_shape = self.env.board.shape[0]\n",
    "    \n",
    "    def predict(self, boardToMove):\n",
    "        # Tiene que devolver la acción en la que come más piezas.\n",
    "        # A igualdad de piezas comidas, samplear uniformemente\n",
    "        valid_actions = self.env.get_valid((boardToMove, self.player))\n",
    "        sampled_valid_action = np.argwhere(valid_actions==self.player)\n",
    "        max_rewards = []\n",
    "        max_actions = []\n",
    "        board = boardToMove\n",
    "        for action in sampled_valid_action:\n",
    "            (board, player), reward, done, info = self.env.next_step((boardToMove, self.player), action)\n",
    "            print(\"Step\", action, reward)\n",
    "            if len(max_rewards) == 0 or reward == max_rewards[0]:\n",
    "                max_rewards += [reward]\n",
    "                max_actions += [action]\n",
    "            if len(max_rewards) > 0 and reward > max_rewards[0]:\n",
    "                max_rewards = [reward]\n",
    "                max_actions = [action]\n",
    "        if len(max_actions) == 0:\n",
    "            return self.env.PASS\n",
    "        else:\n",
    "            return max_actions[randint(0, len(max_actions) -1)]\n",
    "        \n",
    "class RandomPlayer3():\n",
    "    def __init__(self, player=1, board_shape=None, env=None, flatten_action=False):\n",
    "        if (env is None) and (board_shape is None):\n",
    "            print(\"board_shape and env can't be both None\")\n",
    "        if env is None:\n",
    "            env = ReversiEnv(board_shape=board_shape)\n",
    "        self.env = env\n",
    "        self.player = player\n",
    "        self.flatten_action = flatten_action\n",
    "        self.board_shape = self.env.board.shape[0]\n",
    "    \n",
    "    def predict(self, board):\n",
    "        # Muestrea aleatoriamente las acciones válidas\n",
    "        # Puede usar la función creada en la notebook anterior\n",
    "        valid_actions = self.env.get_valid((board, self.player))\n",
    "        sampled_valid_action = np.argwhere(valid_actions==self.player)\n",
    "        if len(sampled_valid_action) == 0:\n",
    "            print (\"No mas movimientos para jugador: \" + str(self.player))\n",
    "            action = self.env.PASS\n",
    "            return action\n",
    "        action = sampled_valid_action[randint(0, len(sampled_valid_action) -1)]\n",
    "        if self.flatten_action:\n",
    "            return action[0] * self.board_shape + action[1]\n",
    "        else:\n",
    "            return action\n",
    "\n",
    "\n",
    "class DictPolicyPlayer3():\n",
    "    def __init__(self, player=1, board_shape=4, env=None, flatten_action=False, dict_folder='mdp/pi_mdp.npy'):\n",
    "        self.pi_dict = np.load(dict_folder, allow_pickle=True).item()\n",
    "        self.env = env\n",
    "        if env is None:\n",
    "            self.env = ReversiEnv(board_shape=board_shape)\n",
    "        self.player = player\n",
    "        self.flatten_action = flatten_action\n",
    "        self.board_shape = board_shape\n",
    "    \n",
    "    def predict(self, board):\n",
    "        # Elegir la acción optima y devolverla\n",
    "        board_tuple = tuple((board * self.player).reshape(-1))\n",
    "        action = self.pi_dict.get(board_tuple, None)\n",
    "        if action is None:\n",
    "            action = self.env.PASS\n",
    "        if not self.flatten_action:\n",
    "            return action\n",
    "        else:\n",
    "            return [action // self.board_shape, action % self.board_shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1675399a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Mi imp\n",
    "#DictPolicyPlayer = DictPolicyPlayer3\n",
    "#RandomPlayer = RandomPlayer3\n",
    "#GreedyPlayer = GreedyPlayer3\n",
    "\n",
    "# Players imp\n",
    "DictPolicyPlayer = DictPolicyPlayer2\n",
    "RandomPlayer = RandomPlayer2\n",
    "GreedyPlayer = GreedyPlayer2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b9733e",
   "metadata": {},
   "source": [
    "# Verificar que el pass funciona OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8228d84c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "gp = GreedyPlayer(player=1, board_shape=4)\n",
    "rp = RandomPlayer(player=1, board_shape=4)\n",
    "board = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, -1, 1, 0],\n",
    "    [0, 1, 1, 0],\n",
    "    [0, 0, 0, 0]]\n",
    ")\n",
    "\n",
    "boardInicial = np.array([\n",
    "    [0, 0, 0, 0],\n",
    "    [0, -1, 1, 0],\n",
    "    [0, 1, -1, 0],\n",
    "    [0, 0, 0, 0]]\n",
    ")\n",
    "\n",
    "boardToMove = np.array([\n",
    "    [0, 0, 0, 0],\n",
    "    [0, -1, -1, 0],\n",
    "    [0, -1, 1, 0],\n",
    "    [1, 0, 0, 0]]\n",
    ")\n",
    "\n",
    "board3 = np.array([\n",
    "    [0, 0, 0, 0],\n",
    "    [0, -1, -1, -1],\n",
    "    [0, 1, -1, 0],\n",
    "    [0, 0, 0, 0]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63dca6dd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  0,  0,  0],\n",
       "       [ 0, -1,  1,  0],\n",
       "       [ 0,  1,  1,  0],\n",
       "       [ 0,  0,  0,  0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(board, player), reward, done, info = gp.env.next_step((boardToMove, 1), [0,2])\n",
    "next_state, _, _, _ = gp.env.next_step((board, gp.player), [0,3])\n",
    "next_state[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34889a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d74690e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dp = DictPolicyPlayer(player=1, board_shape=4, dict_folder='mdp/pi_mdp.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d877bdf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM [0 1]\n",
      "Greedy [0 1]\n",
      "Direct [-1  0]\n"
     ]
    }
   ],
   "source": [
    "print(\"RANDOM\", rp.predict(board))\n",
    "print(\"Greedy\", gp.predict(board))\n",
    "print(\"Direct\", dp.predict(board))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a0e973-3682-480a-9f69-3b381d73cda9",
   "metadata": {},
   "source": [
    "# Completar la función que dado dos jugadores imprima estadísticas de las partidas"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60f47a75-560a-415d-8b34-ae4f897d80fe",
   "metadata": {},
   "source": [
    "Por ejemplo:\n",
    "(Las estadísticas son relativas el que se pasa primero en la función)\n",
    "\n",
    "Wins as first: 0.35\n",
    "Wins as second: 0.55\n",
    "Plays as first: 2457\n",
    "Plays as second: 2543\n",
    "Avg game duration: 5.937"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bef78de3-e74b-41bb-b612-dcab6ec8ab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arena_stats(Player_1, Player_2, board_shape, N=500):\n",
    "    \n",
    "    env = ReversiEnv(board_shape=board_shape)\n",
    "    wins_as_first = 0\n",
    "    wins_as_second = 0\n",
    "    plays_as_first = 0\n",
    "    plays_as_second = 0\n",
    "    total_steps = 0\n",
    "    player_1 = Player_1(player=1, board_shape=board_shape, flatten_action=False)\n",
    "    player_2 = Player_2(player=-1, board_shape=board_shape, flatten_action=False)\n",
    "    for i in range(N):\n",
    "        # Aveces empieza un jugador, a veces el otro\n",
    "        first_player = np.random.choice([-1, 1])\n",
    "        player_1.player = first_player\n",
    "        player_2.player = -first_player\n",
    "        \n",
    "        plays_as_first = plays_as_first + (first_player == 1)\n",
    "        plays_as_second = plays_as_second + (first_player == -1)\n",
    "        \n",
    "        done = False\n",
    "        n_steps = 0\n",
    "        (board, player) = env.reset()\n",
    "        \n",
    "        while not done:\n",
    "            if first_player == player:\n",
    "                action = player_1.predict(board) # Juega el jugador 1\n",
    "            else:\n",
    "                action = player_2.predict(board) # Juega el jugador 2\n",
    "            (board, player), reward, done, info = env.step(action)\n",
    "            n_steps = n_steps + 1\n",
    "        total_steps = total_steps + n_steps\n",
    "        wins_as_first = wins_as_first + (reward == first_player) * (first_player == 1)\n",
    "        wins_as_second = wins_as_second + (reward == first_player) * (first_player == -1)\n",
    "    print(f'Wins as first: {wins_as_first/plays_as_first}')\n",
    "    print(f'Wins as second: {wins_as_second/plays_as_second}')\n",
    "    print(f'Plays as first: {plays_as_first}')\n",
    "    print(f'Plays as second: {plays_as_second}')\n",
    "    print(f'Avg game duration: {total_steps/N}')\n",
    "\n",
    "    return [player_1, player_2, wins_as_first + wins_as_second, total_steps, N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eb85b44-7c8d-4b96-a0fb-a0ab41ff75d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins as first: 0.8399592252803262\n",
      "Wins as second: 1.0\n",
      "Plays as first: 981\n",
      "Plays as second: 1019\n",
      "Avg game duration: 11.7355\n"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "stats += [arena_stats(DictPolicyPlayer, GreedyPlayer, 4, N=2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f83803e3-b172-44f8-9f9b-413c75975a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins as first: 0.8153846153846154\n",
      "Wins as second: 1.0\n",
      "Plays as first: 520\n",
      "Plays as second: 480\n",
      "Avg game duration: 11.664\n"
     ]
    }
   ],
   "source": [
    "stats += [arena_stats(DictPolicyPlayer, RandomPlayer, 4, N=1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a4790ea-396c-4691-925d-1e84441ea129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins as first: 0.0\n",
      "Wins as second: 0.12909441233140656\n",
      "Plays as first: 481\n",
      "Plays as second: 519\n",
      "Avg game duration: 11.646\n"
     ]
    }
   ],
   "source": [
    "stats += [arena_stats(RandomPlayer, DictPolicyPlayer, 4, N=1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c157f772-2a91-40d4-83f4-6f1ca8fb9c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins as first: 0.39644970414201186\n",
      "Wins as second: 0.49898580121703856\n",
      "Plays as first: 507\n",
      "Plays as second: 493\n",
      "Avg game duration: 11.675\n"
     ]
    }
   ],
   "source": [
    "stats += [arena_stats(RandomPlayer, GreedyPlayer, 4, N=1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f3ee8f6-6a86-41a6-8aca-4831ca280897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins as first: 0.36531365313653136\n",
      "Wins as second: 0.5021834061135371\n",
      "Plays as first: 271\n",
      "Plays as second: 229\n",
      "Avg game duration: 11.782\n"
     ]
    }
   ],
   "source": [
    "stats += [arena_stats(RandomPlayer, RandomPlayer, 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2583b3c6-a24a-4737-bf4a-a9cb114f4958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins as first: 0.3953488372093023\n",
      "Wins as second: 0.45867768595041325\n",
      "Plays as first: 258\n",
      "Plays as second: 242\n",
      "Avg game duration: 11.416\n"
     ]
    }
   ],
   "source": [
    "stats += [arena_stats(GreedyPlayer, GreedyPlayer, 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1ae26ae-ad80-4111-8aa5-a24dd32e1773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins as first: 0.38181818181818183\n",
      "Wins as second: 0.32475247524752476\n",
      "Plays as first: 495\n",
      "Plays as second: 505\n",
      "Avg game duration: 57.991\n"
     ]
    }
   ],
   "source": [
    "stats += [arena_stats(RandomPlayer, GreedyPlayer, 8, N=1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e235c-b140-4989-8065-e03bf9fa40ca",
   "metadata": {},
   "source": [
    "# Guardar todas las clases de jugadores en un player.py para que luego se puedan importar de la siguiente forma:\n",
    "\n",
    "from players import RandomPlayer\n",
    "\n",
    "from players import GreedyPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1366c04f-65a4-4464-8cdb-34103fe17d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DictPolicyPlayer (1843 - 92%) vs GreedyPlayer (157)  (moves 11.7)\n",
      "DictPolicyPlayer (904 - 90%) vs RandomPlayer (96)  (moves 11.7)\n",
      "RandomPlayer (67 - 7%) vs DictPolicyPlayer (933)  (moves 11.6)\n",
      "RandomPlayer (447 - 45%) vs GreedyPlayer (553)  (moves 11.7)\n",
      "RandomPlayer (214 - 43%) vs RandomPlayer (286)  (moves 11.8)\n",
      "GreedyPlayer (213 - 43%) vs GreedyPlayer (287)  (moves 11.4)\n",
      "RandomPlayer (353 - 35%) vs GreedyPlayer (647)  (moves 58.0)\n"
     ]
    }
   ],
   "source": [
    "for stat in stats:\n",
    "    print(type(stat[0]).__name__, \"(\" + str(stat[2]) + \" - \" + str(round(100 * stat[2] / stat[4])) + \"%) vs\", type(stat[1]).__name__, \"(\" + str(stat[4] - stat[2]) + \")\", \" (moves\", str(round(stat[3] / stat[4], 1)) + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5329213",
   "metadata": {},
   "source": [
    "DictPolicyPlayer3 (2000 - 100%) vs GreedyPlayer3 (0)  (moves 6.8)\n",
    "DictPolicyPlayer3 (1000 - 100%) vs RandomPlayer3 (0)  (moves 6.9)\n",
    "RandomPlayer3 (0 - 0%) vs DictPolicyPlayer3 (1000)  (moves 6.8)\n",
    "RandomPlayer3 (474 - 47%) vs GreedyPlayer3 (526)  (moves 2.0)\n",
    "RandomPlayer3 (275 - 55%) vs RandomPlayer3 (225)  (moves 2.0)\n",
    "GreedyPlayer3 (247 - 49%) vs GreedyPlayer3 (253)  (moves 2.0)\n",
    "RandomPlayer3 (489 - 49%) vs GreedyPlayer3 (511)  (moves 2.0)\n",
    "\n",
    "DictPolicyPlayer (1829 - 91%) vs GreedyPlayer (171)  (moves 11.7)\n",
    "DictPolicyPlayer (907 - 91%) vs RandomPlayer (93)  (moves 11.7)\n",
    "RandomPlayer (69 - 7%) vs DictPolicyPlayer (931)  (moves 11.6)\n",
    "RandomPlayer (455 - 46%) vs GreedyPlayer (545)  (moves 11.6)\n",
    "RandomPlayer (229 - 46%) vs RandomPlayer (271)  (moves 11.8)\n",
    "GreedyPlayer (236 - 47%) vs GreedyPlayer (264)  (moves 11.6)\n",
    "RandomPlayer (380 - 38%) vs GreedyPlayer (620)  (moves 58.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43c36bc3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date and time = 29/01/2023 00:26:27\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"date and time =\", dt_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
