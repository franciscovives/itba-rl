{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "027229e4-9c75-4418-9111-3d1afc4c5526",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# ! pip install stable_baselines3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d47e401-bb8b-4200-8aa9-afaa856a924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boardgame2 import ReversiEnv\n",
    "import numpy as np\n",
    "from players import RandomPlayer\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74717cb-c130-4894-99ec-548d0f39acf3",
   "metadata": {},
   "source": [
    "# SelfPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f09c2-5a93-4a5e-9e53-a11391af1cea",
   "metadata": {},
   "source": [
    "En esta notebook se pide armar un entorno al cual se le pase como parámetro la clase de jugador local (DictPlayer, RandomPlayer, GreedyPlayer), y que el entorno devuelva el siguiente paso luego de jugar con el jugador local. Algunas condiciones:\n",
    "- En la función de reset(), se sorteará si el jugador local juega primero o segundo.\n",
    "- El entorno siempre devolverá el tablero como si le tocará jugar al jugador 1. Sea primero o segundo\n",
    "- La clase se instancia con los siguientes parámetros:\n",
    "    - LocalPlayer\n",
    "    - board_shape\n",
    "    \n",
    "- El método step recibirá como parámetro la acción pero codificada no como action = [columna, fila], si no como: action = action[0] * board_shape + action[1]\n",
    "- self.action_space tiene que estar definido acorde al espacio de acción. Por ejemplo: self.action_space = gym.spaces.Discrete(board_shape**2)\n",
    "- self.observation_space también: self.observation_space = gym.spaces.Box(-1, 1, (1, board_shape,board_shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1d97d4-db5e-46b7-a273-4fad6bc07373",
   "metadata": {},
   "source": [
    "# Ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03553fc6-987c-4e88-9cf3-777cd4b87843",
   "metadata": {},
   "source": [
    "El jugador local juega segundo entonces el reset() devuelve (Notar que no se devuelve el player por que siempre juega el 1):"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a50ac17-cfa0-452b-9fa2-63a495ffbe6f",
   "metadata": {},
   "source": [
    "[[ 0,  0,  0,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0, -1,  1,  0],\n",
    " [ 0,  0,  0,  0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3090f893-6ede-4b8d-9559-b01341181dee",
   "metadata": {},
   "source": [
    "El jugador local juega primero entonces el reset() devuelve:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7cfa1d6c-c465-46a5-b539-56edd8b6513c",
   "metadata": {},
   "source": [
    "[[ 0,  0, -1,  0],\n",
    " [ 0, -1, -1,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0,  0,  0,  0]]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac37d6d6-e177-4cc7-b1f3-f136e6bb40fd",
   "metadata": {},
   "source": [
    "Que ocurrio aca?\n",
    "\n",
    "El tablero se resetea y queda:\n",
    "\n",
    "[[ 0,  0,  0,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0, -1,  1,  0],\n",
    " [ 0,  0,  0,  0]]\n",
    " \n",
    "Luego el jugador local muestrea una de las cuatros opciones válidas y juega (0, 2) comiendo la pieza (1, 2) y tranformandola en 1\n",
    "\n",
    "[[ 0,  0,  1,  0],\n",
    " [ 0,  1,  1,  0],\n",
    " [ 0, -1,  1,  0],\n",
    " [ 0,  0,  0,  0]]\n",
    " \n",
    "Ahora le toca el turno al jugador -1 pero el jugador externo tiene que ver el tablero como si fuera el 1, entonces se multiplica el tablero por -1\n",
    "\n",
    "[[ 0,  0, -1,  0],\n",
    " [ 0, -1, -1,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0,  0,  0,  0]]\n",
    " \n",
    "Ahora el jugador externo seleccionará una acción observando el tablero como si fuera 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f918a12-4d72-48cb-84b0-617054ec02a3",
   "metadata": {},
   "source": [
    "En cuanto a la recompenza tener en cuenta que deberá devolver:\n",
    "- 1 si gana el jugador externo\n",
    "- -1 si gana el LocalPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from multi_env import SelfPlayEnv as SelfPlayEnv3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef1f596-1a0e-449f-86f1-5c5115a00d17",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ReversiEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-cc701342d564>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mclass\u001B[0m \u001B[0mSelfPlayEnv2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mReversiEnv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mboard_shape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m8\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mLocalPlayer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplayers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlocal_player\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mLocalPlayer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mboard_shape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mboard_shape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mflatten_action\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ReversiEnv' is not defined"
     ]
    }
   ],
   "source": [
    "class SelfPlayEnv2(ReversiEnv):\n",
    "    def __init__(self, board_shape=8, LocalPlayer=None):\n",
    "        self.players = [-1, 1]\n",
    "\n",
    "        self.local_player = LocalPlayer(board_shape=board_shape, flatten_action=False)\n",
    "        self.board_shape = board_shape\n",
    "        super(SelfPlayEnv, self).__init__(board_shape=board_shape)\n",
    "        \n",
    "        self.action_space = 1 # TODO Implementar\n",
    "        self.observation_space = 1 # TODO Implementar\n",
    "        self.action_space = gym.spaces.Discrete(board_shape**2)\n",
    "        self.observation_space = gym.spaces.Box(-1, 1, (1, board_shape,board_shape)) # aca\n",
    "         \n",
    "        \n",
    "    def play(self, observation):\n",
    "        print(\"Playing \" + str(observation))\n",
    "        action = self.local_player.predict(observation)\n",
    "        (observation, self.current_player_num), reward, done, info = super(SelfPlayEnv, self).step(action)\n",
    "\n",
    "        return (observation, self.current_player_num), reward, done, info\n",
    "    \n",
    "    def encode_observation(self, observation, valid_actions=False):\n",
    "        # TODO Implementar\n",
    "        # Simpre devuelve desde el punto de vista del jugador 1\n",
    "        # No devuleve el jugador sino solo el tablero\n",
    "        # Tener en cuenta que esto será la entrada a la red neuronal\n",
    "        board = observation * self.current_player_num\n",
    "        if valid_actions:\n",
    "            return np.array([board, self.get_valid((board, 1))])\n",
    "        else:\n",
    "            return observation * self.current_player_num\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.n_step = 0\n",
    "        self.local_player_num = np.random.choice(self.players)\n",
    "        self.local_player.player = self.local_player_num\n",
    "        self.observation, self.current_player_num = super(SelfPlayEnv, self).reset()\n",
    "        print(\"Obs\", self.observation, \"Player\", self.current_player_num)\n",
    "        self.allow_pass = True\n",
    "\n",
    "            \n",
    "        if self.current_player_num == self.local_player_num:   \n",
    "            (self.observation, self.current_player_num), _, done, info = self.play(self.observation)\n",
    "            assert done == False\n",
    "\n",
    "        \n",
    "        return self.encode_observation(self.observation)\n",
    "    \n",
    "    def encode_action(self, action):\n",
    "        # Esta es necesario ya que la salida de la red neuronal será un valor entre 0 y board_shape**2 - 1\n",
    "        return [action // self.board_shape, action % self.board_shape]\n",
    "    \n",
    "    def decode_action(self, action):\n",
    "        return action[0] * self.board_shape + action[1]\n",
    "\n",
    "    def step(self, action):\n",
    "        self.n_step += 1\n",
    "        action = self.encode_action(action)\n",
    "        \n",
    "        (self.observation, self.current_player_num), reward, done, _ = super(SelfPlayEnv, self).step(action)   \n",
    "        \n",
    "            \n",
    "        while not done and (self.current_player_num == self.local_player_num):            \n",
    "            (self.observation, self.current_player_num), reward, done, info = self.play(self.observation)\n",
    "\n",
    "        \n",
    "        encoded_observation = self.encode_observation(self.observation)\n",
    "        reward = 1 # TODO Implementar. Tiene que ser un float\n",
    "        reward = -float(self.local_player_num*reward)\n",
    "\n",
    "        return encoded_observation, reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SelfPlayEnv = SelfPlayEnv3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce3799d-d2bd-4590-b076-25e742c235f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SelfPlayEnv(board_shape=4, LocalPlayer=RandomPlayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4a7bd-0b0c-47e4-a63f-465fa83226de",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c2352-7ff1-455c-b2e8-b0a9844dd1f7",
   "metadata": {},
   "source": [
    "# Probar entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d80054a-c061-4973-9a95-ab76fa91a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_valid_actions(state):\n",
    "    # np.argwhere junto con env.get_valid y randint solucionan el problema en pocas lineas pero puede usar otra estrategia\n",
    "    board_shape = state.shape[0]\n",
    "    # El player es siempre 1\n",
    "    player = 1\n",
    "    valid_actions = np.argwhere(env.get_valid((state, player)) == 1)\n",
    "    action = valid_actions[np.random.randint(len(valid_actions))]\n",
    "    return action[0] * board_shape + action[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f074437d-a34d-4bdf-a122-164c1ee63123",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "board = env.reset()\n",
    "print(\"Board\", board)\n",
    "while not done:\n",
    "    action = sample_valid_actions(board)\n",
    "    print(board)\n",
    "    print(f'acion: {action}')\n",
    "    \n",
    "    board, reward, done, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc2a60-1484-43cf-9cfe-3bc435aaa548",
   "metadata": {},
   "source": [
    "# Entornos vectoriales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9342c-4c16-4fa2-aed7-a591e4fad79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_env import make_reversi_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dfb931b-9602-489f-b52c-4266b054b78c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_reversi_vec_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-8f588dd3f9dc>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mboard_shape\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m8\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mn_envs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m env = make_reversi_vec_env(\n\u001B[0m\u001B[1;32m      4\u001B[0m     \u001B[0mSelfPlayEnv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_envs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mn_envs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mmonitor_dir\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'monitor'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'make_reversi_vec_env' is not defined"
     ]
    }
   ],
   "source": [
    "board_shape = 8\n",
    "n_envs = 10\n",
    "env = make_reversi_vec_env(\n",
    "    SelfPlayEnv, n_envs=n_envs,\n",
    "    monitor_dir='monitor',\n",
    "    env_kwargs={\n",
    "        'board_shape': board_shape,\n",
    "        'LocalPlayer': RandomPlayer\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb45e843-724f-4318-942e-7b581909d973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode [[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 -1  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]]\n",
      "Encode [[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 -1  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]]\n",
      "Playing [[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 -1  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]]\n",
      "Encode [[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  1  1  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]]\n",
      "Playing [[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 -1  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]]\n",
      "Encode [[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 -1  0  0  0]\n",
      " [ 0  0  0  1  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]]\n",
      "Encode [[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 -1  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]]\n",
      "Playing [[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 -1  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]]\n",
      "Encode [[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 -1  0  0  0]\n",
      " [ 0  0  0  1  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]]\n",
      "Playing [[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 -1  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]]\n",
      "Encode [[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 -1  0  0  0]\n",
      " [ 0  0  1  1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]]\n",
      "Playing [[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 -1  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]]\n",
      "Encode [[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 -1  0  0  0]\n",
      " [ 0  0  1  1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]]\n",
      "Playing [[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 -1  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]]\n",
      "Encode [[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 -1  0  0  0]\n",
      " [ 0  0  0  1  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]]\n",
      "Encode [[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 -1  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "(10, 1, 8, 8)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6172549d-264f-4e31-bba9-939f5a1cd61d",
   "metadata": {},
   "source": [
    "- Notar que la entrada tiene como primer componente la cantidad de entornos en paralelo (10), luego la cantidad de canales (1), y finalmente las dimensiones del tablero \n",
    "\n",
    "- Imprimir obs y ver que hay distintas posibles entradas dependiendo de quien juega primero y que jugó el LocalPlayer si le toco primero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c95848-d644-435e-af95-cf754690dfa1",
   "metadata": {},
   "source": [
    "### Guardar el SelfPlayEnv en el módulo multi_env para poder despues importarla desde otra notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c447b-def1-46e6-8bf3-299e7e596d2c",
   "metadata": {},
   "source": [
    "# Instanciamos el modelo con MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16ebead4-efc5-498e-a411-e8f1ef955ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym.spaces\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ecdb1b-7588-488b-b4ed-5722f2fb2555",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ac1fd0-947c-4e8d-b42a-5b18ffc3f687",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd324ce9-6a80-437e-aedd-b44585c99afa",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c274276c-25eb-4b94-9841-89e00795fe49",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6736603d-2624-4e97-8d32-dcb251ff1586",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model.predict(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1497081e-e277-4dff-b576-ae58f96ac85b",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "- Lo primero que hace stablebaselines si ponemos MLP es un flatten\n",
    "- Las acciones predichas por el modelo (sin entrentar) tienen una alta probabildad de ser inválidas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
